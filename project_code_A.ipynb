{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.ndimage # Added for mask resizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Config\n",
        "# =========================\n",
        "class Config:\n",
        "    data_root = \"data/Sports\"  # expects data_root/train and data_root/val\n",
        "    image_size = 64            # 32 or 64\n",
        "    batch_size = 64\n",
        "    num_workers = 4\n",
        "\n",
        "    num_classes = 10\n",
        "    num_epochs = 10\n",
        "    lr = 1e-3\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    out_dir = \"outputs_problem_a\"\n",
        "    model_a_name = \"simple_cnn\"\n",
        "    model_b_name = \"small_resnet\"\n",
        "\n",
        "    run_name_a = \"run_model_A_simple_cnn\"\n",
        "    run_name_b = \"run_model_B_small_resnet\"\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "os.makedirs(cfg.out_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Data loaders\n",
        "# =========================\n",
        "def get_dataloaders(cfg: Config):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((cfg.image_size, cfg.image_size)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((cfg.image_size, cfg.image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    train_dir = os.path.join(cfg.data_root, \"train\")\n",
        "    val_dir = os.path.join(cfg.data_root, \"valid\") # Changed 'val' to 'valid'\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "    val_dataset = datasets.ImageFolder(val_dir, transform=val_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=cfg.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=cfg.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    class_names = train_dataset.classes\n",
        "    return train_loader, val_loader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Model A - Simple CNN\n",
        "# =========================\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(2),  # 64 -> 32\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(2),  # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(2),  # 16 -> 8\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 8 * 8, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Model B - Small ResNet style\n",
        "# =========================\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=3,\n",
        "            stride=stride, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels, out_channels, kernel_size=3,\n",
        "            stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels, out_channels,\n",
        "                    kernel_size=1, stride=stride, bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        out = out + identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SmallResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.layer1 = BasicBlock(32, 64, stride=2)   # 64 -> 32\n",
        "        self.layer2 = BasicBlock(64, 128, stride=2)  # 32 -> 16\n",
        "        self.layer3 = BasicBlock(128, 256, stride=2) # 16 -> 8\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Utility\n",
        "# =========================\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Train and eval loops\n",
        "# =========================\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += preds.eq(targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc, epoch_time\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device, num_classes, class_names=None):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    per_class_correct = np.zeros(num_classes, dtype=np.int64)\n",
        "    per_class_total = np.zeros(num_classes, dtype=np.int64)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "\n",
        "            correct += preds.eq(targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "            for t, p in zip(targets, preds):\n",
        "                per_class_total[t.item()] += 1\n",
        "                if t.item() == p.item():\n",
        "                    per_class_correct[t.item()] += 1\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    per_class_acc = per_class_correct / np.maximum(per_class_total, 1)\n",
        "\n",
        "    if class_names is not None:\n",
        "        print(\"Per class accuracy:\")\n",
        "        for idx, name in enumerate(class_names):\n",
        "            print(f\"{name}: {per_class_acc[idx] * 100:.2f}% \"\n",
        "                  f\"({per_class_correct[idx]}/{per_class_total[idx]})\")\n",
        "\n",
        "    return epoch_loss, epoch_acc, per_class_acc\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Saliency maps\n",
        "# =========================\n",
        "def compute_saliency_map(model, image_tensor, label, device):\n",
        "    model.eval()\n",
        "    image = image_tensor.unsqueeze(0).to(device)\n",
        "    image.requires_grad_()\n",
        "\n",
        "    output = model(image)\n",
        "    target_score = output[0, label]\n",
        "    model.zero_grad()\n",
        "    target_score.backward()\n",
        "\n",
        "    saliency = image.grad.data.abs().max(dim=1)[0]  # shape (1, H, W)\n",
        "    saliency = saliency.squeeze(0).cpu().numpy() # Now shape (H, W)\n",
        "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)\n",
        "    return saliency\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Grad CAM\n",
        "# =========================\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "\n",
        "        self.hook_a = target_layer.register_forward_hook(self.save_activation)\n",
        "        self.hook_g = target_layer.register_full_backward_hook(self.save_gradient)\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        self.activations = output.detach()\n",
        "\n",
        "    def save_gradient(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0].detach()\n",
        "\n",
        "    def __call__(self, input_tensor, target_class=None):\n",
        "        self.model.eval()\n",
        "        input_tensor = input_tensor.unsqueeze(0)\n",
        "        input_tensor = input_tensor.to(next(self.model.parameters()).device)\n",
        "        input_tensor.requires_grad_()\n",
        "\n",
        "        output = self.model(input_tensor)\n",
        "        if target_class is None:\n",
        "            target_class = output.argmax(dim=1).item()\n",
        "\n",
        "        loss = output[0, target_class]\n",
        "        self.model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        gradients = self.gradients  # shape (1, C, H, W)\n",
        "        activations = self.activations  # shape (1, C, H, W)\n",
        "\n",
        "        # Corrected: average gradients over spatial dimensions (H, W) for each channel and batch item\n",
        "        weights = gradients.mean(dim=(2, 3), keepdim=True)  # shape (1, C, 1, 1)\n",
        "\n",
        "        # Corrected: multiply weights with activations and sum over channel dimension, then squeeze batch dim\n",
        "        cam = (weights * activations).sum(dim=1)  # shape (1, H, W)\n",
        "        cam = cam.squeeze(0)  # shape (H, W)\n",
        "\n",
        "        cam = cam.cpu().numpy()\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "        return cam\n",
        "\n",
        "    def close(self):\n",
        "        self.hook_a.remove()\n",
        "        self.hook_g.remove()\n",
        "\n",
        "\n",
        "def show_and_save_heatmap(image_tensor, mask, out_path, alpha=0.4):\n",
        "    # image_tensor is normalized. Need to unnormalize for visualization.\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n",
        "\n",
        "    img = image_tensor.cpu().numpy()\n",
        "    img = std * img + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "    img = np.transpose(img, (1, 2, 0)) # Shape (H, W, C)\n",
        "\n",
        "    # Resize mask to the same dimensions as the image if needed\n",
        "    img_h, img_w, _ = img.shape\n",
        "    mask_h, mask_w = mask.shape\n",
        "\n",
        "    if mask_h != img_h or mask_w != img_w:\n",
        "        zoom_factor_h = img_h / mask_h\n",
        "        zoom_factor_w = img_w / mask_w\n",
        "        # Use order=1 for bilinear interpolation\n",
        "        mask = scipy.ndimage.zoom(mask, (zoom_factor_h, zoom_factor_w), order=1)\n",
        "\n",
        "    heatmap = plt.cm.jet(mask)[..., :3]\n",
        "    overlay = alpha * heatmap + (1 - alpha) * img\n",
        "    overlay = np.clip(overlay, 0, 1)\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(overlay)\n",
        "    plt.tight_layout()\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    plt.savefig(out_path, bbox_inches=\"tight\", pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Training wrapper\n",
        "# =========================\n",
        "def train_and_eval_model(model, model_name, run_name, cfg, train_loader, val_loader, class_names):\n",
        "    device = cfg.device\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    writer = SummaryWriter(log_dir=os.path.join(cfg.out_dir, run_name))\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    num_params = count_parameters(model)\n",
        "    print(f\"Model {model_name} param count: {num_params}\")\n",
        "\n",
        "    for epoch in range(cfg.num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{cfg.num_epochs}\")\n",
        "\n",
        "        train_loss, train_acc, train_time = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, device\n",
        "        )\n",
        "        val_loss, val_acc, _ = evaluate(\n",
        "            model, val_loader, criterion, device,\n",
        "            cfg.num_classes, class_names=None\n",
        "        )\n",
        "\n",
        "        print(f\"Train loss {train_loss:.4f} acc {train_acc:.4f} \"\n",
        "              f\"time {train_time:.2f}s\")\n",
        "        print(f\"Val   loss {val_loss:.4f} acc {val_acc:.4f}\")\n",
        "\n",
        "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
        "        writer.add_scalar(\"Acc/train\", train_acc, epoch)\n",
        "        writer.add_scalar(\"Acc/val\", val_acc, epoch)\n",
        "        writer.add_scalar(\"Time/train_epoch_sec\", train_time, epoch)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            checkpoint_path = os.path.join(\n",
        "                cfg.out_dir, f\"{model_name}-best-original.pt\"\n",
        "            )\n",
        "            torch.save(best_model_wts, checkpoint_path)\n",
        "            print(f\"Saved best checkpoint to {checkpoint_path}\")\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    final_ckpt_path = os.path.join(\n",
        "        cfg.out_dir, f\"{model_name}-final-original.pt\"\n",
        "    )\n",
        "    torch.save(model.state_dict(), final_ckpt_path)\n",
        "    print(f\"Saved final model to {final_ckpt_path}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    val_loss, val_acc, per_class_acc = evaluate(\n",
        "        model, val_loader, criterion, device,\n",
        "        cfg.num_classes, class_names=class_names\n",
        "    )\n",
        "\n",
        "    print(f\"Best val accuracy for {model_name}: {best_val_acc:.4f}\")\n",
        "    print(f\"Final val accuracy for {model_name}: {val_acc:.4f}\")\n",
        "\n",
        "    return model, per_class_acc\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Run interpretability\n",
        "# =========================\n",
        "def run_interpretability(model, cfg, class_names, val_loader, model_name):\n",
        "    device = cfg.device\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # choose last conv layer as target for Grad CAM\n",
        "    if isinstance(model, SimpleCNN):\n",
        "        target_layer = model.features[-3]  # last Conv in features\n",
        "    elif isinstance(model, SmallResNet):\n",
        "        target_layer = model.layer3  # last residual block\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model type for Grad CAM\")\n",
        "\n",
        "    grad_cam = GradCAM(model, target_layer)\n",
        "\n",
        "    out_dir = Path(cfg.out_dir) / f\"{model_name}_interpretability\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    images_done = 0\n",
        "    max_images = 10  # you can change\n",
        "\n",
        "    # Removed with torch.no_grad(): block here\n",
        "    for inputs, targets in val_loader:\n",
        "        for i in range(inputs.size(0)):\n",
        "            img = inputs[i]\n",
        "            label = targets[i].item()\n",
        "            label_name = class_names[label]\n",
        "\n",
        "            # Saliency uses grad, so need to re run with grad\n",
        "            sal_map = compute_saliency_map(model, img, label, device)\n",
        "\n",
        "            # Grad CAM\n",
        "            cam = grad_cam(img, target_class=label)\n",
        "\n",
        "            # save saliency\n",
        "            sal_out_path = out_dir / f\"saliency_{images_done}_{label_name}.png\"\n",
        "            plt.figure(figsize=(4, 4))\n",
        "            plt.axis(\"off\")\n",
        "            plt.imshow(sal_map, cmap=\"hot\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(sal_out_path, bbox_inches=\"tight\", pad_inches=0)\n",
        "            plt.close()\n",
        "\n",
        "            # save Grad CAM overlay\n",
        "            cam_out_path = out_dir / f\"gradcam_{images_done}_{label_name}.png\"\n",
        "            show_and_save_heatmap(img, cam, str(cam_out_path))\n",
        "\n",
        "            images_done += 1\n",
        "            if images_done >= max_images:\n",
        "                grad_cam.close()\n",
        "                print(f\"Saved {images_done} interpretability images for {model_name}\")\n",
        "                return\n",
        "\n",
        "    grad_cam.close()\n",
        "    print(f\"Saved {images_done} interpretability images for {model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Model A - SimpleCNN\n",
            "Model simple_cnn param count: 4586506\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\packj\\OneDrive\\Documents\\School\\LSU-8 Fall 2025\\EE 4745\\EE4745_Final_Proj\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss 3.1326 acc 0.2524 time 9.52s\n",
            "Val   loss 1.6791 acc 0.3600\n",
            "Saved best checkpoint to outputs_problem_a\\simple_cnn-best-original.pt\n",
            "Epoch 2/10\n",
            "Train loss 1.8161 acc 0.3333 time 8.41s\n",
            "Val   loss 1.4393 acc 0.4600\n",
            "Saved best checkpoint to outputs_problem_a\\simple_cnn-best-original.pt\n",
            "Epoch 3/10\n",
            "Train loss 1.6625 acc 0.4018 time 7.98s\n",
            "Val   loss 1.3412 acc 0.4600\n",
            "Epoch 4/10\n",
            "Train loss 1.5934 acc 0.4275 time 8.02s\n",
            "Val   loss 1.4398 acc 0.5200\n",
            "Saved best checkpoint to outputs_problem_a\\simple_cnn-best-original.pt\n",
            "Epoch 5/10\n",
            "Train loss 1.4985 acc 0.4614 time 8.14s\n",
            "Val   loss 1.1254 acc 0.6200\n",
            "Saved best checkpoint to outputs_problem_a\\simple_cnn-best-original.pt\n",
            "Epoch 6/10\n",
            "Train loss 1.4944 acc 0.4520 time 8.13s\n",
            "Val   loss 1.2493 acc 0.5800\n",
            "Epoch 7/10\n",
            "Train loss 1.4060 acc 0.5028 time 8.13s\n",
            "Val   loss 1.1639 acc 0.5400\n",
            "Epoch 8/10\n",
            "Train loss 1.4019 acc 0.4947 time 7.80s\n",
            "Val   loss 1.1382 acc 0.5800\n",
            "Epoch 9/10\n",
            "Train loss 1.3245 acc 0.5185 time 8.36s\n",
            "Val   loss 1.0635 acc 0.6200\n",
            "Epoch 10/10\n",
            "Train loss 1.3091 acc 0.5204 time 8.11s\n",
            "Val   loss 1.0283 acc 0.6600\n",
            "Saved best checkpoint to outputs_problem_a\\simple_cnn-best-original.pt\n",
            "Saved final model to outputs_problem_a\\simple_cnn-final-original.pt\n",
            "Per class accuracy:\n",
            "baseball: 60.00% (3/5)\n",
            "basketball: 60.00% (3/5)\n",
            "football: 60.00% (3/5)\n",
            "golf: 100.00% (5/5)\n",
            "hockey: 100.00% (5/5)\n",
            "rugby: 40.00% (2/5)\n",
            "swimming: 100.00% (5/5)\n",
            "tennis: 60.00% (3/5)\n",
            "volleyball: 20.00% (1/5)\n",
            "weightlifting: 60.00% (3/5)\n",
            "Best val accuracy for simple_cnn: 0.6600\n",
            "Final val accuracy for simple_cnn: 0.6600\n",
            "Saved 10 interpretability images for simple_cnn\n",
            "\n",
            "Training Model B - SmallResNet\n",
            "Model small_resnet param count: 1210410\n",
            "Epoch 1/10\n",
            "Train loss 1.7071 acc 0.3980 time 9.02s\n",
            "Val   loss 1.9164 acc 0.4600\n",
            "Saved best checkpoint to outputs_problem_a\\small_resnet-best-original.pt\n",
            "Epoch 2/10\n",
            "Train loss 1.3504 acc 0.5386 time 8.64s\n",
            "Val   loss 1.5836 acc 0.4800\n",
            "Saved best checkpoint to outputs_problem_a\\small_resnet-best-original.pt\n",
            "Epoch 3/10\n",
            "Train loss 1.2300 acc 0.5838 time 9.99s\n",
            "Val   loss 1.1986 acc 0.5600\n",
            "Saved best checkpoint to outputs_problem_a\\small_resnet-best-original.pt\n",
            "Epoch 4/10\n",
            "Train loss 1.0988 acc 0.6303 time 9.06s\n",
            "Val   loss 1.2271 acc 0.6600\n",
            "Saved best checkpoint to outputs_problem_a\\small_resnet-best-original.pt\n",
            "Epoch 5/10\n",
            "Train loss 1.0829 acc 0.6265 time 8.79s\n",
            "Val   loss 1.0836 acc 0.7000\n",
            "Saved best checkpoint to outputs_problem_a\\small_resnet-best-original.pt\n",
            "Epoch 6/10\n",
            "Train loss 0.9806 acc 0.6654 time 9.32s\n",
            "Val   loss 0.9822 acc 0.6400\n",
            "Epoch 7/10\n",
            "Train loss 0.9228 acc 0.6880 time 8.71s\n",
            "Val   loss 1.1614 acc 0.5600\n",
            "Epoch 8/10\n",
            "Train loss 0.8602 acc 0.7006 time 8.96s\n",
            "Val   loss 1.0273 acc 0.6800\n",
            "Epoch 9/10\n",
            "Train loss 0.8225 acc 0.7263 time 9.05s\n",
            "Val   loss 0.8585 acc 0.6200\n",
            "Epoch 10/10\n",
            "Train loss 0.7464 acc 0.7433 time 8.78s\n",
            "Val   loss 1.0191 acc 0.6600\n",
            "Saved final model to outputs_problem_a\\small_resnet-final-original.pt\n",
            "Per class accuracy:\n",
            "baseball: 80.00% (4/5)\n",
            "basketball: 60.00% (3/5)\n",
            "football: 40.00% (2/5)\n",
            "golf: 100.00% (5/5)\n",
            "hockey: 80.00% (4/5)\n",
            "rugby: 40.00% (2/5)\n",
            "swimming: 100.00% (5/5)\n",
            "tennis: 100.00% (5/5)\n",
            "volleyball: 20.00% (1/5)\n",
            "weightlifting: 80.00% (4/5)\n",
            "Best val accuracy for small_resnet: 0.7000\n",
            "Final val accuracy for small_resnet: 0.7000\n",
            "Saved 10 interpretability images for small_resnet\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =========================\n",
        "# Main\n",
        "# =========================\n",
        "def main():\n",
        "    train_loader, val_loader, class_names = get_dataloaders(cfg)\n",
        "\n",
        "    print(\"Training Model A - SimpleCNN\")\n",
        "    model_a = SimpleCNN(num_classes=cfg.num_classes)\n",
        "    model_a, per_class_acc_a = train_and_eval_model(\n",
        "        model_a,\n",
        "        cfg.model_a_name,\n",
        "        cfg.run_name_a,\n",
        "        cfg,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        class_names\n",
        "    )\n",
        "    run_interpretability(model_a, cfg, class_names, val_loader, cfg.model_a_name)\n",
        "\n",
        "    print(\"\\nTraining Model B - SmallResNet\")\n",
        "    model_b = SmallResNet(num_classes=cfg.num_classes)\n",
        "    model_b, per_class_acc_b = train_and_eval_model(\n",
        "        model_b,\n",
        "        cfg.model_b_name,\n",
        "        cfg.run_name_b,\n",
        "        cfg,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        class_names\n",
        "    )\n",
        "    run_interpretability(model_b, cfg, class_names, val_loader, cfg.model_b_name)\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading outputs_problem_a\\simple_cnn-best-original.pt\n",
            "Loading outputs_problem_a\\small_resnet-best-original.pt\n",
            "\n",
            "====================================\n",
            "Attack fgsm, targeted=False\n",
            "Generating on Model A (SimpleCNN)\n",
            "Model A fgsm untargeted success 0.300 on 10 examples\n",
            "Transfer fgsm untargeted success rate on other model 0.300\n",
            "Generating on Model B (SmallResNet)\n",
            "Model B fgsm untargeted success 0.500 on 10 examples\n",
            "Transfer fgsm untargeted success rate on other model 0.200\n",
            "\n",
            "====================================\n",
            "Attack fgsm, targeted=True\n",
            "Generating on Model A (SimpleCNN)\n",
            "Model A fgsm targeted success 0.300 on 10 examples\n",
            "Transfer fgsm targeted success rate on other model 0.200\n",
            "Generating on Model B (SmallResNet)\n",
            "Model B fgsm targeted success 0.300 on 10 examples\n",
            "Transfer fgsm targeted success rate on other model 0.300\n",
            "\n",
            "====================================\n",
            "Attack pgd, targeted=False\n",
            "Generating on Model A (SimpleCNN)\n",
            "Model A pgd untargeted success 0.500 on 10 examples\n",
            "Transfer pgd untargeted success rate on other model 0.300\n",
            "Generating on Model B (SmallResNet)\n",
            "Model B pgd untargeted success 0.500 on 10 examples\n",
            "Transfer pgd untargeted success rate on other model 0.200\n",
            "\n",
            "====================================\n",
            "Attack pgd, targeted=True\n",
            "Generating on Model A (SimpleCNN)\n",
            "Model A pgd targeted success 0.400 on 10 examples\n",
            "Transfer pgd targeted success rate on other model 0.200\n",
            "Generating on Model B (SmallResNet)\n",
            "Model B pgd targeted success 0.300 on 10 examples\n",
            "Transfer pgd targeted success rate on other model 0.300\n",
            "\n",
            "Done Part B\n"
          ]
        }
      ],
      "source": [
        "# part B\n",
        "\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.ndimage # Added for mask resizing\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Config\n",
        "# =========================\n",
        "class ConfigB:\n",
        "    data_root = \"data/Sports\"  # same as Part A, changed to capital 'S'\n",
        "    image_size = 64\n",
        "    batch_size = 32\n",
        "    num_workers = 4\n",
        "    num_classes = 10\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # model checkpoint paths from Part A\n",
        "    ckpt_dir = \"outputs_problem_a\"\n",
        "    simple_ckpt = \"simple_cnn-best-original.pt\"\n",
        "    resnet_ckpt = \"small_resnet-best-original.pt\"\n",
        "\n",
        "    out_dir = \"outputs_problem_b\"\n",
        "\n",
        "    # attack settings\n",
        "    eps_fgsm = 0.03          # epsilon in normalized space\n",
        "    eps_pgd = 0.03\n",
        "    pgd_alpha = 0.007\n",
        "    pgd_steps = 20\n",
        "\n",
        "    # target label index for \"basketball\"\n",
        "    basketball_class_name = \"basketball\"\n",
        "\n",
        "\n",
        "cfg = ConfigB()\n",
        "os.makedirs(cfg.out_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Dataset and loader\n",
        "# =========================\n",
        "def get_val_loader(cfg: ConfigB):\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((cfg.image_size, cfg.image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    val_dir = os.path.join(cfg.data_root, \"valid\") # Changed 'val' to 'valid'\n",
        "    val_dataset = datasets.ImageFolder(val_dir, transform=val_transform)\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=cfg.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    return val_loader, val_dataset.classes\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Models copied from Part A\n",
        "# =========================\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 8 * 8, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=3,\n",
        "            stride=stride, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels, out_channels, kernel_size=3,\n",
        "            stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels, out_channels,\n",
        "                    kernel_size=1, stride=stride, bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        out = out + identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SmallResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.layer1 = BasicBlock(32, 64, stride=2)\n",
        "        self.layer2 = BasicBlock(64, 128, stride=2)\n",
        "        self.layer3 = BasicBlock(128, 256, stride=2)\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def load_models(cfg: ConfigB, class_names):\n",
        "    device = cfg.device\n",
        "\n",
        "    model_a = SimpleCNN(num_classes=cfg.num_classes).to(device)\n",
        "    model_b = SmallResNet(num_classes=cfg.num_classes).to(device)\n",
        "\n",
        "    ckpt_a = os.path.join(cfg.ckpt_dir, cfg.simple_ckpt)\n",
        "    ckpt_b = os.path.join(cfg.ckpt_dir, cfg.resnet_ckpt)\n",
        "\n",
        "    print(f\"Loading {ckpt_a}\")\n",
        "    model_a.load_state_dict(torch.load(ckpt_a, map_location=device))\n",
        "    print(f\"Loading {ckpt_b}\")\n",
        "    model_b.load_state_dict(torch.load(ckpt_b, map_location=device))\n",
        "\n",
        "    # find index for basketball\n",
        "    if cfg.basketball_class_name in class_names:\n",
        "        basketball_idx = class_names.index(cfg.basketball_class_name)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"basketball class not found. classes are {class_names}\"\n",
        "        )\n",
        "\n",
        "    return model_a, model_b, basketball_idx\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Interpretability helpers\n",
        "# =========================\n",
        "def compute_saliency_map(model, image_tensor, label, device):\n",
        "    model.eval()\n",
        "    image = image_tensor.unsqueeze(0).to(device)\n",
        "    image.requires_grad_()\n",
        "\n",
        "    output = model(image)\n",
        "    target_score = output[0, label]\n",
        "    model.zero_grad()\n",
        "    target_score.backward()\n",
        "\n",
        "    saliency = image.grad.data.abs().max(dim=1)[0]\n",
        "    saliency = saliency.squeeze(0).cpu().numpy()\n",
        "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)\n",
        "    return saliency\n",
        "\n",
        "\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "        self.hook_a = target_layer.register_forward_hook(self.save_activation)\n",
        "        self.hook_g = target_layer.register_full_backward_hook(self.save_gradient)\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        self.activations = output.detach()\n",
        "\n",
        "    def save_gradient(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0].detach()\n",
        "\n",
        "    def __call__(self, input_tensor, target_class=None):\n",
        "        self.model.eval()\n",
        "        input_tensor = input_tensor.unsqueeze(0)\n",
        "        input_tensor = input_tensor.to(next(self.model.parameters()).device)\n",
        "        input_tensor.requires_grad_()\n",
        "\n",
        "        output = self.model(input_tensor)\n",
        "        if target_class is None:\n",
        "            target_class = output.argmax(dim=1).item()\n",
        "\n",
        "        loss = output[0, target_class]\n",
        "        self.model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        gradients = self.gradients\n",
        "        activations = self.activations\n",
        "\n",
        "        weights = gradients.mean(dim=(2, 3), keepdim=True)  # Corrected: average over spatial dimensions (H, W)\n",
        "        cam = (weights * activations).sum(dim=1)\n",
        "        cam = cam.squeeze(0)  # Add squeeze to remove batch dimension\n",
        "\n",
        "        cam = cam.cpu().numpy()\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "        return cam\n",
        "\n",
        "    def close(self):\n",
        "        self.hook_a.remove()\n",
        "        self.hook_g.remove()\n",
        "\n",
        "\n",
        "def show_and_save_overlay(image_tensor, heatmap, out_path, alpha=0.4):\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n",
        "\n",
        "    img = image_tensor.cpu().numpy()\n",
        "    img = std * img + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "    img = np.transpose(img, (1, 2, 0)) # Shape (H, W, C)\n",
        "\n",
        "    # Resize heatmap to the same dimensions as the image\n",
        "    img_h, img_w, _ = img.shape\n",
        "    mask_h, mask_w = heatmap.shape\n",
        "\n",
        "    if mask_h != img_h or mask_w != img_w:\n",
        "        zoom_factor_h = img_h / mask_h\n",
        "        zoom_factor_w = img_w / mask_w\n",
        "        # Use order=1 for bilinear interpolation\n",
        "        heatmap = scipy.ndimage.zoom(heatmap, (zoom_factor_h, zoom_factor_w), order=1)\n",
        "\n",
        "    heat = plt.cm.jet(heatmap)[..., :3]\n",
        "    overlay = alpha * heat + (1 - alpha) * img\n",
        "    overlay = np.clip(overlay, 0, 1)\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(overlay)\n",
        "    plt.tight_layout()\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    plt.savefig(out_path, bbox_inches=\"tight\", pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_clean_and_adv_maps(model, img_clean, img_adv, label_true, label_adv,\n",
        "                            class_names, out_dir, prefix):\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    label_true_name = class_names[label_true]\n",
        "    label_adv_name = class_names[label_adv]\n",
        "\n",
        "    # choose last conv layer\n",
        "    if isinstance(model, SimpleCNN):\n",
        "        target_layer = model.features[-3]\n",
        "    elif isinstance(model, SmallResNet):\n",
        "        target_layer = model.layer3\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model type for GradCAM\")\n",
        "\n",
        "    grad_cam = GradCAM(model, target_layer)\n",
        "\n",
        "    # saliency clean\n",
        "    sal_clean = compute_saliency_map(model, img_clean, label_true, device)\n",
        "    sal_adv = compute_saliency_map(model, img_adv, label_adv, device)\n",
        "\n",
        "    # grad cam clean and adv\n",
        "    cam_clean = grad_cam(img_clean, target_class=label_true)\n",
        "    cam_adv = grad_cam(img_adv, target_class=label_adv)\n",
        "\n",
        "    # save\n",
        "    base = Path(out_dir)\n",
        "    base.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # raw saliency\n",
        "    for name, sal in [(\"clean\", sal_clean), (\"adv\", sal_adv)]:\n",
        "        p = base / f\"{prefix}_saliency_{name}_{label_true_name}_to_{label_adv_name}.png\"\n",
        "        plt.figure(figsize=(4, 4))\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(sal, cmap=\"hot\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(p, bbox_inches=\"tight\", pad_inches=0)\n",
        "        plt.close()\n",
        "\n",
        "    # overlays\n",
        "    show_and_save_overlay(\n",
        "        img_clean, cam_clean,\n",
        "        str(base / f\"{prefix}_gradcam_clean_{label_true_name}.png\")\n",
        "    )\n",
        "    show_and_save_overlay(\n",
        "        img_adv, cam_adv,\n",
        "        str(base / f\"{prefix}_gradcam_adv_{label_adv_name}.png\")\n",
        "    )\n",
        "\n",
        "    grad_cam.close()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Attack functions\n",
        "# =========================\n",
        "def clamp_tensor(x, mean, std):\n",
        "    # x is normalized. we clamp in image space then renormalize\n",
        "    mean = torch.tensor(mean, device=x.device).view(1, 3, 1, 1)\n",
        "    std = torch.tensor(std, device=x.device).view(1, 3, 1, 1)\n",
        "    img = x * std + mean\n",
        "    img = torch.clamp(img, 0.0, 1.0)\n",
        "    x_norm = (img - mean) / std\n",
        "    return x_norm\n",
        "\n",
        "\n",
        "def fgsm_attack(model, x, y, eps, targeted=False, target_labels=None):\n",
        "    model.eval()\n",
        "    x_adv = x.clone().detach().requires_grad_(True)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    outputs = model(x_adv)\n",
        "    if targeted:\n",
        "        assert target_labels is not None\n",
        "        loss = criterion(outputs, target_labels)\n",
        "        grad_sign = torch.sign(torch.autograd.grad(loss, x_adv)[0])\n",
        "        x_adv = x_adv - eps * grad_sign\n",
        "    else:\n",
        "        loss = criterion(outputs, y)\n",
        "        grad_sign = torch.sign(torch.autograd.grad(loss, x_adv)[0])\n",
        "        x_adv = x_adv + eps * grad_sign\n",
        "\n",
        "    x_adv = x_adv.detach()\n",
        "    x_adv = clamp_tensor(\n",
        "        x_adv,\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "    return x_adv\n",
        "\n",
        "\n",
        "def pgd_attack(model, x, y, eps, alpha, steps, targeted=False, target_labels=None):\n",
        "    model.eval()\n",
        "    x_orig = x.clone().detach()\n",
        "    x_adv = x_orig + 0.001 * torch.randn_like(x_orig)\n",
        "    x_adv.requires_grad_(True)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for _ in range(steps):\n",
        "        outputs = model(x_adv)\n",
        "        if targeted:\n",
        "            assert target_labels is not None\n",
        "            loss = criterion(outputs, target_labels)\n",
        "            grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "            x_adv = x_adv - alpha * torch.sign(grad)\n",
        "        else:\n",
        "            loss = criterion(outputs, y)\n",
        "            grad = torch.autograd.grad(loss, x_adv)[0]\n",
        "            x_adv = x_adv + alpha * torch.sign(grad)\n",
        "\n",
        "        # project back to epsilon ball\n",
        "        eta = torch.clamp(x_adv - x_orig, min=-eps, max=eps)\n",
        "        x_adv = x_orig + eta\n",
        "        x_adv = clamp_tensor(\n",
        "            x_adv,\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "        x_adv = x_adv.detach()\n",
        "        x_adv.requires_grad_(True)\n",
        "\n",
        "    return x_adv.detach()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Generate and evaluate adversarials\n",
        "# =========================\n",
        "def generate_adversarial_set(\n",
        "    model,\n",
        "    val_loader,\n",
        "    attack_type,\n",
        "    eps,\n",
        "    targeted,\n",
        "    target_class_idx,\n",
        "    num_samples,\n",
        "    alpha=None,\n",
        "    steps=None\n",
        "):\n",
        "    device = cfg.device\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    collected = []\n",
        "\n",
        "    for inputs, targets in val_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # only keep correctly classified to start\n",
        "        with torch.no_grad():\n",
        "            logits_clean = model(inputs)\n",
        "            preds_clean = logits_clean.argmax(dim=1)\n",
        "        mask_correct = preds_clean.eq(targets)\n",
        "        if mask_correct.sum().item() == 0:\n",
        "            continue\n",
        "\n",
        "        inputs_c = inputs[mask_correct]\n",
        "        targets_c = targets[mask_correct]\n",
        "        logits_clean = logits_clean[mask_correct]\n",
        "        preds_clean = preds_clean[mask_correct]\n",
        "\n",
        "        if targeted:\n",
        "            target_labels = torch.full_like(\n",
        "                targets_c, target_class_idx, device=device\n",
        "            )\n",
        "        else:\n",
        "            target_labels = None\n",
        "\n",
        "        if attack_type == \"fgsm\":\n",
        "            adv = fgsm_attack(\n",
        "                model, inputs_c, targets_c, eps,\n",
        "                targeted=targeted, target_labels=target_labels\n",
        "            )\n",
        "        elif attack_type == \"pgd\":\n",
        "            adv = pgd_attack(\n",
        "                model, inputs_c, targets_c, eps,\n",
        "                alpha=alpha, steps=steps,\n",
        "                targeted=targeted, target_labels=target_labels\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\"Unknown attack type\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits_adv = model(adv)\n",
        "            preds_adv = logits_adv.argmax(dim=1)\n",
        "\n",
        "        for i in range(inputs_c.size(0)):\n",
        "            x0 = inputs_c[i].detach().cpu()\n",
        "            xa = adv[i].detach().cpu()\n",
        "            y_true = int(targets_c[i].item())\n",
        "            y_clean = int(preds_clean[i].item())\n",
        "            y_adv = int(preds_adv[i].item())\n",
        "\n",
        "            lc = logits_clean[i].detach().cpu().numpy()\n",
        "            la = logits_adv[i].detach().cpu().numpy()\n",
        "\n",
        "            diff = (xa - x0).view(-1)\n",
        "            l2 = torch.norm(diff, p=2).item()\n",
        "            linf = torch.norm(diff, p=float(\"inf\")).item()\n",
        "\n",
        "            if targeted:\n",
        "                success = (y_adv == target_class_idx)\n",
        "            else:\n",
        "                success = (y_adv != y_true)\n",
        "\n",
        "            collected.append({\n",
        "                \"x_clean\": x0,\n",
        "                \"x_adv\": xa,\n",
        "                \"y_true\": y_true,\n",
        "                \"y_pred_clean\": y_clean,\n",
        "                \"y_pred_adv\": y_adv,\n",
        "                \"logits_clean\": lc,\n",
        "                \"logits_adv\": la,\n",
        "                \"l2\": l2,\n",
        "                \"linf\": linf,\n",
        "                \"success\": success,\n",
        "            })\n",
        "\n",
        "            if len(collected) >= num_samples:\n",
        "                return collected\n",
        "\n",
        "    return collected\n",
        "\n",
        "\n",
        "def save_example_images(examples, class_names, out_dir, prefix):\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n",
        "\n",
        "    for idx, ex in enumerate(examples):\n",
        "        x0 = ex[\"x_clean\"].numpy()\n",
        "        xa = ex[\"x_adv\"].numpy()\n",
        "\n",
        "        def denorm(x):\n",
        "            img = std * x + mean\n",
        "            img = np.clip(img, 0, 1)\n",
        "            img = np.transpose(img, (1, 2, 0))\n",
        "            return img\n",
        "\n",
        "        img0 = denorm(x0)\n",
        "        imga = denorm(xa)\n",
        "\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
        "        axs[0].imshow(img0)\n",
        "        axs[0].axis(\"off\")\n",
        "        axs[0].set_title(\n",
        "            f\"Clean {class_names[ex['y_true']]} / pred {class_names[ex['y_pred_clean']]}\"\n",
        "        )\n",
        "        axs[1].imshow(imga)\n",
        "        axs[1].axis(\"off\")\n",
        "        axs[1].set_title(\n",
        "            f\"Adv pred {class_names[ex['y_pred_adv']]}\"\n",
        "        )\n",
        "        plt.tight_layout()\n",
        "        fig.savefig(out_dir / f\"{prefix}_pair_{idx}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "def transferability_test(examples, model_other, class_names, attack_name, targeted, target_idx):\n",
        "    device = cfg.device\n",
        "    model_other.to(device)\n",
        "    model_other.eval()\n",
        "\n",
        "    total = 0\n",
        "    success = 0\n",
        "\n",
        "    for ex in examples:\n",
        "        xa = ex[\"x_adv\"].unsqueeze(0).to(device)\n",
        "        y_true = ex[\"y_true\"]\n",
        "        with torch.no_grad():\n",
        "            logits = model_other(xa)\n",
        "            pred = logits.argmax(dim=1).item()\n",
        "\n",
        "        if targeted:\n",
        "            suc = (pred == target_idx)\n",
        "        else:\n",
        "            suc = (pred != y_true)\n",
        "\n",
        "        success += int(suc)\n",
        "        total += 1\n",
        "\n",
        "    rate = success / max(total, 1)\n",
        "    t_type = \"targeted\" if targeted else \"untargeted\"\n",
        "    print(f\"Transfer {attack_name} {t_type} success rate on other model {rate:.3f}\")\n",
        "    return rate\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Main experiment\n",
        "# =========================\n",
        "def main():\n",
        "    val_loader, class_names = get_val_loader(cfg)\n",
        "    model_a, model_b, basketball_idx = load_models(cfg, class_names)\n",
        "\n",
        "    # attacks to run\n",
        "    attacks = [\n",
        "        (\"fgsm\", False),\n",
        "        (\"fgsm\", True),\n",
        "        (\"pgd\", False),\n",
        "        (\"pgd\", True),\n",
        "    ]\n",
        "\n",
        "    for attack_name, targeted in attacks:\n",
        "        print(\"\\n====================================\")\n",
        "        print(f\"Attack {attack_name}, targeted={targeted}\")\n",
        "        print(\"Generating on Model A (SimpleCNN)\")\n",
        "\n",
        "        adv_a = generate_adversarial_set(\n",
        "            model_a,\n",
        "            val_loader,\n",
        "            attack_type=attack_name,\n",
        "            eps=cfg.eps_fgsm if attack_name == \"fgsm\" else cfg.eps_pgd,\n",
        "            targeted=targeted,\n",
        "            target_class_idx=basketball_idx,\n",
        "            num_samples=10,\n",
        "            alpha=cfg.pgd_alpha if attack_name == \"pgd\" else None,\n",
        "            steps=cfg.pgd_steps if attack_name == \"pgd\" else None,\n",
        "        )\n",
        "\n",
        "        success_rate_a = sum(int(e[\"success\"]) for e in adv_a) / max(len(adv_a), 1)\n",
        "        atype = \"targeted\" if targeted else \"untargeted\"\n",
        "        print(f\"Model A {attack_name} {atype} success {success_rate_a:.3f} \"\n",
        "              f\"on {len(adv_a)} examples\")\n",
        "\n",
        "        # save images\n",
        "        prefix_a = f\"modelA_{attack_name}_{'targeted' if targeted else 'untargeted'}\"\n",
        "        save_example_images(\n",
        "            adv_a,\n",
        "            class_names,\n",
        "            os.path.join(cfg.out_dir, \"examples\"),\n",
        "            prefix_a\n",
        "        )\n",
        "\n",
        "        # interpretability for first few\n",
        "        inter_dir = os.path.join(cfg.out_dir, \"interpretability\")\n",
        "        for i, ex in enumerate(adv_a[:3]):\n",
        "            save_clean_and_adv_maps(\n",
        "                model_a,\n",
        "                ex[\"x_clean\"],\n",
        "                ex[\"x_adv\"],\n",
        "                ex[\"y_true\"],\n",
        "                ex[\"y_pred_adv\"],\n",
        "                class_names,\n",
        "                inter_dir,\n",
        "                prefix=f\"{prefix_a}_ex{i}\"\n",
        "            )\n",
        "\n",
        "        # transferability from model A to B\n",
        "        transferability_test(\n",
        "            adv_a, model_b, class_names, attack_name, targeted, basketball_idx\n",
        "        )\n",
        "\n",
        "        # now generate on Model B\n",
        "        print(\"Generating on Model B (SmallResNet)\")\n",
        "\n",
        "        adv_b = generate_adversarial_set(\n",
        "            model_b,\n",
        "            val_loader,\n",
        "            attack_type=attack_name,\n",
        "            eps=cfg.eps_fgsm if attack_name == \"fgsm\" else cfg.eps_pgd,\n",
        "            targeted=targeted,\n",
        "            target_class_idx=basketball_idx,\n",
        "            num_samples=10,\n",
        "            alpha=cfg.pgd_alpha if attack_name == \"pgd\" else None,\n",
        "            steps=cfg.pgd_steps if attack_name == \"pgd\" else None,\n",
        "        )\n",
        "\n",
        "        success_rate_b = sum(int(e[\"success\"]) for e in adv_b) / max(len(adv_b), 1)\n",
        "        atype = \"targeted\" if targeted else \"untargeted\"\n",
        "        print(f\"Model B {attack_name} {atype} success {success_rate_b:.3f} \"\n",
        "              f\"on {len(adv_b)} examples\")\n",
        "\n",
        "        prefix_b = f\"modelB_{attack_name}_{'targeted' if targeted else 'untargeted'}\"\n",
        "        save_example_images(\n",
        "            adv_b,\n",
        "            class_names,\n",
        "            os.path.join(cfg.out_dir, \"examples\"),\n",
        "            prefix_b\n",
        "        )\n",
        "\n",
        "        for i, ex in enumerate(adv_b[:3]):\n",
        "            save_clean_and_adv_maps(\n",
        "                model_b,\n",
        "                ex[\"x_clean\"],\n",
        "                ex[\"x_adv\"],\n",
        "                ex[\"y_true\"],\n",
        "                ex[\"y_pred_adv\"],\n",
        "                class_names,\n",
        "                inter_dir,\n",
        "                prefix=f\"{prefix_b}_ex{i}\"\n",
        "            )\n",
        "\n",
        "        # transferability from model B to A\n",
        "        transferability_test(\n",
        "            adv_b, model_a, class_names, attack_name, targeted, basketball_idx\n",
        "        )\n",
        "\n",
        "    print(\"\\nDone Part B\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading baseline from outputs_problem_a\\simple_cnn-best-original.pt\n",
            "\n",
            "Baseline model summary\n",
            "Val loss 1.0283 acc 0.6600\n",
            "Params 4586506\n",
            "File size 17.51 MB\n",
            "Sparsity 0.0000\n",
            "Latency bs=1 2.055 +- 0.470 ms\n",
            "Latency bs=16 18.815 +- 1.519 ms\n",
            "\n",
            "Collected 20 examples for adversarial eval\n",
            "Baseline adv untargeted success 0.300 on 20 examples\n",
            "Baseline adv targeted success 0.200 on 20 examples\n",
            "\n",
            "==============================\n",
            "Pruning sparsity 0.2\n",
            "Actual sparsity 0.2000\n",
            "After prune before finetune val_loss 1.1536 val_acc 0.6200\n",
            "Finetune epoch 1 train_loss 1.4474 train_acc 0.4600 val_loss 1.0889 val_acc 0.6800\n",
            "Finetune epoch 2 train_loss 1.3486 train_acc 0.4800 val_loss 1.0446 val_acc 0.6800\n",
            "Finetune epoch 3 train_loss 1.3980 train_acc 0.4400 val_loss 1.0050 val_acc 0.7000\n",
            "Finetune epoch 4 train_loss 1.2131 train_acc 0.6000 val_loss 0.9725 val_acc 0.7200\n",
            "Finetune epoch 5 train_loss 1.2208 train_acc 0.5000 val_loss 0.9466 val_acc 0.7400\n",
            "After finetune val_loss 0.9466 val_acc 0.7400\n",
            "Pruned 20% adv untargeted success 0.300 on 20 examples\n",
            "Pruned 20% adv targeted success 0.200 on 20 examples\n",
            "\n",
            "==============================\n",
            "Pruning sparsity 0.5\n",
            "Actual sparsity 0.4999\n",
            "After prune before finetune val_loss 1.3522 val_acc 0.5400\n",
            "Finetune epoch 1 train_loss 1.6135 train_acc 0.4800 val_loss 1.2939 val_acc 0.5400\n",
            "Finetune epoch 2 train_loss 1.4014 train_acc 0.4600 val_loss 1.2465 val_acc 0.6000\n",
            "Finetune epoch 3 train_loss 1.4564 train_acc 0.4200 val_loss 1.2044 val_acc 0.6400\n",
            "Finetune epoch 4 train_loss 1.5677 train_acc 0.4600 val_loss 1.1682 val_acc 0.6600\n",
            "Finetune epoch 5 train_loss 1.2849 train_acc 0.5200 val_loss 1.1353 val_acc 0.6600\n",
            "After finetune val_loss 1.1682 val_acc 0.6600\n",
            "Pruned 50% adv untargeted success 0.412 on 17 examples\n",
            "Pruned 50% adv targeted success 0.176 on 17 examples\n",
            "\n",
            "==============================\n",
            "Pruning sparsity 0.8\n",
            "Actual sparsity 0.7999\n",
            "After prune before finetune val_loss 2.7440 val_acc 0.1400\n",
            "Finetune epoch 1 train_loss 2.5200 train_acc 0.2800 val_loss 2.7270 val_acc 0.1200\n",
            "Finetune epoch 2 train_loss 2.1311 train_acc 0.3200 val_loss 2.6935 val_acc 0.1200\n",
            "Finetune epoch 3 train_loss 2.1555 train_acc 0.3000 val_loss 2.6410 val_acc 0.1200\n",
            "Finetune epoch 4 train_loss 2.2401 train_acc 0.2400 val_loss 2.5756 val_acc 0.1400\n",
            "Finetune epoch 5 train_loss 1.9360 train_acc 0.2600 val_loss 2.4989 val_acc 0.1400\n",
            "After finetune val_loss 2.5756 val_acc 0.1400\n",
            "Pruned 80% adv untargeted success 0.167 on 6 examples\n",
            "Pruned 80% adv targeted success 0.000 on 6 examples\n",
            "\n",
            "Saved summary CSV and plots to outputs_problem_c\n"
          ]
        }
      ],
      "source": [
        "#part C \n",
        "import os\n",
        "import time\n",
        "from pathlib import Path\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Config\n",
        "# =========================\n",
        "class ConfigC:\n",
        "    data_root = \"data/Sports\"\n",
        "    image_size = 64\n",
        "    batch_size = 64\n",
        "    num_workers = 4\n",
        "    num_classes = 10\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # choose which model to prune: \"simple\" or \"resnet\"\n",
        "    model_type = \"simple\"  # change to \"resnet\" if you prefer\n",
        "\n",
        "    ckpt_dir = \"outputs_problem_a\"\n",
        "    simple_ckpt = \"simple_cnn-best-original.pt\"\n",
        "    resnet_ckpt = \"small_resnet-best-original.pt\"\n",
        "\n",
        "    out_dir = \"outputs_problem_c\"\n",
        "\n",
        "    # pruning levels\n",
        "    sparsity_levels = [0.2, 0.5, 0.8]\n",
        "\n",
        "    # fine tune\n",
        "    finetune_epochs = 5\n",
        "    finetune_lr = 1e-4\n",
        "    finetune_weight_decay = 1e-5\n",
        "\n",
        "    # adversarial settings (reuse style from Part B)\n",
        "    eps_fgsm = 0.03\n",
        "    basketball_class_name = \"basketball\"\n",
        "    num_adv_eval = 20  # number of samples for robustness check\n",
        "\n",
        "\n",
        "cfg = ConfigC()\n",
        "os.makedirs(cfg.out_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Dataset\n",
        "# =========================\n",
        "def get_val_loader(cfg: ConfigC):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((cfg.image_size, cfg.image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        ),\n",
        "    ])\n",
        "    val_dir = os.path.join(cfg.data_root, \"valid\")\n",
        "    val_ds = datasets.ImageFolder(val_dir, transform=transform)\n",
        "    val_loader = DataLoader(\n",
        "        val_ds,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=cfg.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    return val_loader, val_ds.classes\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Models (same as Part A)\n",
        "# =========================\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 8 * 8, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=3,\n",
        "            stride=stride, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels, out_channels, kernel_size=3,\n",
        "            stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels, out_channels,\n",
        "                    kernel_size=1, stride=stride, bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "        out = out + identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SmallResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.layer1 = BasicBlock(32, 64, stride=2)\n",
        "        self.layer2 = BasicBlock(64, 128, stride=2)\n",
        "        self.layer3 = BasicBlock(128, 256, stride=2)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def load_baseline_model(cfg: ConfigC, class_names):\n",
        "    device = cfg.device\n",
        "    if cfg.model_type == \"simple\":\n",
        "        model = SimpleCNN(num_classes=cfg.num_classes)\n",
        "        ckpt_path = os.path.join(cfg.ckpt_dir, cfg.simple_ckpt)\n",
        "    elif cfg.model_type == \"resnet\":\n",
        "        model = SmallResNet(num_classes=cfg.num_classes)\n",
        "        ckpt_path = os.path.join(cfg.ckpt_dir, cfg.resnet_ckpt)\n",
        "    else:\n",
        "        raise ValueError(\"model_type must be 'simple' or 'resnet'\")\n",
        "\n",
        "    print(f\"Loading baseline from {ckpt_path}\")\n",
        "    state_dict = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model = model.to(device)\n",
        "\n",
        "    if cfg.basketball_class_name in class_names:\n",
        "        basketball_idx = class_names.index(cfg.basketball_class_name)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"basketball class not found in {class_names}\"\n",
        "        )\n",
        "    return model, basketball_idx\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Evaluation and metrics\n",
        "# =========================\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = crit(logits, y)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            total_correct += preds.eq(y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    acc = total_correct / total\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "def model_sparsity(model):\n",
        "    total = 0\n",
        "    zeros = 0\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "            if hasattr(module, 'weight') and isinstance(module.weight, torch.Tensor):\n",
        "                total += module.weight.numel()\n",
        "                zeros += (module.weight == 0).sum().item()\n",
        "            if hasattr(module, 'bias') and isinstance(module.bias, torch.Tensor):\n",
        "                total += module.bias.numel()\n",
        "                zeros += (module.bias == 0).sum().item()\n",
        "    if total == 0: # Avoid division by zero if no prunable layers are found\n",
        "        return 0.0\n",
        "    return zeros / total\n",
        "\n",
        "\n",
        "def save_model_and_get_size(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
        "    return size_mb\n",
        "\n",
        "\n",
        "def measure_latency(model, device, batch_size, num_runs=100, warmup=10):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    x = torch.randn(batch_size, 3, cfg.image_size, cfg.image_size, device=device)\n",
        "\n",
        "    # warm up\n",
        "    with torch.no_grad():\n",
        "        for _ in range(warmup):\n",
        "            _ = model(x)\n",
        "\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_runs):\n",
        "            start = time.perf_counter()\n",
        "            _ = model(x)\n",
        "            if device.type == \"cuda\":\n",
        "                torch.cuda.synchronize()\n",
        "            end = time.perf_counter()\n",
        "            times.append((end - start) * 1000.0)  # ms\n",
        "\n",
        "    times = np.array(times)\n",
        "    return times.mean(), times.std()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Pruning\n",
        "# =========================\n",
        "def prune_model_unstructured(model, amount):\n",
        "    # prune conv and linear layers\n",
        "    for module in model.modules():\n",
        "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "            prune.l1_unstructured(module, name=\"weight\", amount=amount)\n",
        "            prune.remove(module, \"weight\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def finetune(model, train_loader, val_loader, cfg: ConfigC):\n",
        "    device = cfg.device\n",
        "    model.train() # Set model to training mode\n",
        "    model.to(device)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    opt = torch.optim.Adam(\n",
        "        model.parameters(), lr=cfg.finetune_lr,\n",
        "        weight_decay=cfg.finetune_weight_decay\n",
        "    )\n",
        "\n",
        "    best_state = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(cfg.finetune_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        total = 0\n",
        "        total_correct = 0\n",
        "\n",
        "        for x, y in train_loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = crit(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            total_correct += preds.eq(y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        train_loss = total_loss / total\n",
        "        train_acc = total_correct / total\n",
        "\n",
        "        val_loss, val_acc = evaluate(model, val_loader, device)\n",
        "        print(f\"Finetune epoch {epoch+1} \"\n",
        "              f\"train_loss {train_loss:.4f} train_acc {train_acc:.4f} \"\n",
        "              f\"val_loss {val_loss:.4f} val_acc {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return model\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Adversarial attacks (simple FGSM)\n",
        "# =========================\n",
        "def clamp_tensor(x):\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406], device=x.device).view(1, 3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225], device=x.device).view(1, 3, 1, 1)\n",
        "    img = x * std + mean\n",
        "    img = torch.clamp(img, 0.0, 1.0)\n",
        "    x_norm = (img - mean) / std\n",
        "    return x_norm\n",
        "\n",
        "\n",
        "def fgsm_attack(model, x, y, eps, targeted=False, target_labels=None):\n",
        "    device = cfg.device\n",
        "    model.eval()\n",
        "    x_adv = x.clone().detach().requires_grad_(True)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "    logits = model(x_adv)\n",
        "    if targeted:\n",
        "        assert target_labels is not None\n",
        "        loss = crit(logits, target_labels)\n",
        "        grad_sign = torch.sign(torch.autograd.grad(loss, x_adv)[0])\n",
        "        x_adv = x_adv - eps * grad_sign\n",
        "    else:\n",
        "        loss = crit(logits, y)\n",
        "        grad_sign = torch.sign(torch.autograd.grad(loss, x_adv)[0])\n",
        "        x_adv = x_adv + eps * grad_sign\n",
        "\n",
        "    x_adv = x_adv.detach()\n",
        "    x_adv = clamp_tensor(x_adv)\n",
        "    return x_adv\n",
        "\n",
        "\n",
        "def build_adv_eval_set(model, val_loader, basketball_idx, num_samples):\n",
        "    device = cfg.device\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    collected = []\n",
        "\n",
        "    for x, y in val_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            preds = logits.argmax(dim=1)\n",
        "\n",
        "        mask = preds.eq(y)\n",
        "        if mask.sum().item() == 0:\n",
        "            continue\n",
        "\n",
        "        x_good = x[mask]\n",
        "        y_good = y[mask]\n",
        "\n",
        "        for i in range(x_good.size(0)):\n",
        "            collected.append((x_good[i].detach().cpu(), int(y_good[i].item())))\n",
        "            if len(collected) >= num_samples:\n",
        "                return collected\n",
        "\n",
        "    return collected\n",
        "\n",
        "\n",
        "def eval_adv_success(model, examples, basketball_idx, eps):\n",
        "    device = cfg.device\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    untargeted_total = 0\n",
        "    untargeted_success = 0\n",
        "\n",
        "    targeted_total = 0\n",
        "    targeted_success = 0\n",
        "\n",
        "    for img_cpu, y_true in examples:\n",
        "        x = img_cpu.unsqueeze(0).to(device)\n",
        "        y = torch.tensor([y_true], device=device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            pred_clean = logits.argmax(dim=1).item()\n",
        "        if pred_clean != y_true:\n",
        "            continue  # ensure clean correct\n",
        "\n",
        "        # untargeted\n",
        "        x_unt = fgsm_attack(model, x, y, eps, targeted=False)\n",
        "        with torch.no_grad():\n",
        "            logits_unt = model(x_unt)\n",
        "            pred_unt = logits_unt.argmax(dim=1).item()\n",
        "        untargeted_total += 1\n",
        "        if pred_unt != y_true:\n",
        "            untargeted_success += 1\n",
        "\n",
        "        # targeted to basketball\n",
        "        target_label = torch.tensor([basketball_idx], device=device)\n",
        "        x_tar = fgsm_attack(model, x, y, eps, targeted=True, target_labels=target_label)\n",
        "        with torch.no_grad():\n",
        "            logits_tar = model(x_tar)\n",
        "            pred_tar = logits_tar.argmax(dim=1).item()\n",
        "        targeted_total += 1\n",
        "        if pred_tar == basketball_idx:\n",
        "            targeted_success += 1\n",
        "\n",
        "    unt_rate = untargeted_success / max(untargeted_total, 1)\n",
        "    tar_rate = targeted_success / max(targeted_total, 1)\n",
        "    return unt_rate, tar_rate, untargeted_total, targeted_total\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Main\n",
        "# =========================\n",
        "def main():\n",
        "    # train loader for finetune uses same transform as val\n",
        "    val_loader, class_names = get_val_loader(cfg)\n",
        "\n",
        "    # reuse val loader as both train and val for finetuning\n",
        "    train_loader = val_loader\n",
        "\n",
        "    baseline_model, basketball_idx = load_baseline_model(cfg, class_names)\n",
        "\n",
        "    device = cfg.device\n",
        "    baseline_model.to(device)\n",
        "\n",
        "    # baseline metrics\n",
        "    base_loss, base_acc = evaluate(baseline_model, val_loader, device)\n",
        "    base_params = count_parameters(baseline_model)\n",
        "    base_path = os.path.join(cfg.out_dir, f\"{cfg.model_type}_baseline.pt\")\n",
        "    base_size = save_model_and_get_size(baseline_model, base_path)\n",
        "    base_sparsity = model_sparsity(baseline_model)\n",
        "    lat1_mean, lat1_std = measure_latency(baseline_model, device, batch_size=1)\n",
        "    lat16_mean, lat16_std = measure_latency(baseline_model, device, batch_size=16)\n",
        "\n",
        "    print(\"\\nBaseline model summary\")\n",
        "    print(f\"Val loss {base_loss:.4f} acc {base_acc:.4f}\")\n",
        "    print(f\"Params {base_params}\")\n",
        "    print(f\"File size {base_size:.2f} MB\")\n",
        "    print(f\"Sparsity {base_sparsity:.4f}\")\n",
        "    print(f\"Latency bs=1 {lat1_mean:.3f} +- {lat1_std:.3f} ms\")\n",
        "    print(f\"Latency bs=16 {lat16_mean:.3f} +- {lat16_std:.3f} ms\")\n",
        "\n",
        "    # build fixed adversarial eval set off baseline\n",
        "    adv_examples_base = build_adv_eval_set(\n",
        "        baseline_model, val_loader, basketball_idx, cfg.num_adv_eval\n",
        "    )\n",
        "    print(f\"\\nCollected {len(adv_examples_base)} examples for adversarial eval\")\n",
        "\n",
        "    base_unt_rate, base_tar_rate, b_u_n, b_t_n = eval_adv_success(\n",
        "        baseline_model, adv_examples_base, basketball_idx, cfg.eps_fgsm\n",
        "    )\n",
        "    print(f\"Baseline adv untargeted success {base_unt_rate:.3f} \"\n",
        "          f\"on {b_u_n} examples\")\n",
        "    print(f\"Baseline adv targeted success {base_tar_rate:.3f} \"\n",
        "          f\"on {b_t_n} examples\")\n",
        "\n",
        "    # record results\n",
        "    results = []\n",
        "    results.append({\n",
        "        \"sparsity\": 0.0,\n",
        "        \"acc_pre\": base_acc,\n",
        "        \"acc_post\": base_acc,\n",
        "        \"params\": base_params,\n",
        "        \"size_mb\": base_size,\n",
        "        \"lat1_mean\": lat1_mean,\n",
        "        \"lat1_std\": lat1_std,\n",
        "        \"lat16_mean\": lat16_mean,\n",
        "        \"lat16_std\": lat16_std,\n",
        "        \"adv_unt\": base_unt_rate,\n",
        "        \"adv_tar\": base_tar_rate,\n",
        "    })\n",
        "\n",
        "    # pruning experiments\n",
        "    for s in cfg.sparsity_levels:\n",
        "        print(\"\\n==============================\")\n",
        "        print(f\"Pruning sparsity {s}\")\n",
        "        model_s = copy.deepcopy(baseline_model)\n",
        "\n",
        "        # pre finetune eval\n",
        "        loss_before, acc_before = evaluate(model_s, val_loader, device)\n",
        "\n",
        "        # apply pruning\n",
        "        model_s = prune_model_unstructured(model_s, amount=s)\n",
        "        sparsity_actual = model_sparsity(model_s)\n",
        "        print(f\"Actual sparsity {sparsity_actual:.4f}\")\n",
        "\n",
        "        # eval after prune but before finetune\n",
        "        loss_pruned, acc_pruned = evaluate(model_s, val_loader, device)\n",
        "        print(f\"After prune before finetune val_loss {loss_pruned:.4f} \"\n",
        "              f\"val_acc {acc_pruned:.4f}\")\n",
        "\n",
        "        # finetune\n",
        "        model_s = finetune(model_s, train_loader, val_loader, cfg)\n",
        "        loss_after, acc_after = evaluate(model_s, val_loader, device)\n",
        "        print(f\"After finetune val_loss {loss_after:.4f} \"\n",
        "              f\"val_acc {acc_after:.4f}\")\n",
        "\n",
        "        # param count and size\n",
        "        params_s = count_parameters(model_s)\n",
        "        path_s = os.path.join(\n",
        "            cfg.out_dir,\n",
        "            f\"{cfg.model_type}_pruned_{int(s*100)}.pt\"\n",
        "        )\n",
        "        size_s = save_model_and_get_size(model_s, path_s)\n",
        "\n",
        "        # latency\n",
        "        lat1_m, lat1_s = measure_latency(model_s, device, batch_size=1)\n",
        "        lat16_m, lat16_s = measure_latency(model_s, device, batch_size=16)\n",
        "\n",
        "        # adv robustness\n",
        "        adv_unt, adv_tar, n_u, n_t = eval_adv_success(\n",
        "            model_s, adv_examples_base, basketball_idx, cfg.eps_fgsm\n",
        "        )\n",
        "        print(f\"Pruned {int(s*100)}% adv untargeted success {adv_unt:.3f} \"\n",
        "              f\"on {n_u} examples\")\n",
        "        print(f\"Pruned {int(s*100)}% adv targeted success {adv_tar:.3f} \"\n",
        "              f\"on {n_t} examples\")\n",
        "\n",
        "        results.append({\n",
        "            \"sparsity\": sparsity_actual,\n",
        "            \"acc_pre\": acc_pruned,\n",
        "            \"acc_post\": acc_after,\n",
        "            \"params\": params_s,\n",
        "            \"size_mb\": size_s,\n",
        "            \"lat1_mean\": lat1_m,\n",
        "            \"lat1_std\": lat1_s,\n",
        "            \"lat16_mean\": lat16_m,\n",
        "            \"lat16_std\": lat16_s,\n",
        "            \"adv_unt\": adv_unt,\n",
        "            \"adv_tar\": adv_tar,\n",
        "        })\n",
        "\n",
        "    # summary table to csv\n",
        "    import csv\n",
        "    csv_path = os.path.join(cfg.out_dir, f\"{cfg.model_type}_pruning_summary.csv\")\n",
        "    with open(csv_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\n",
        "            \"sparsity\", \"acc_pre\", \"acc_post\", \"params\", \"size_mb\",\n",
        "            \"lat1_mean\", \"lat1_std\", \"lat16_mean\", \"lat16_std\",\n",
        "            \"adv_unt\", \"adv_tar\"\n",
        "        ])\n",
        "        for r in results:\n",
        "            writer.writerow([\n",
        "                r[\"sparsity\"], r[\"acc_pre\"], r[\"acc_post\"], r[\"params\"],\n",
        "                r[\"size_mb\"], r[\"lat1_mean\"], r[\"lat1_std\"],\n",
        "                r[\"lat16_mean\"], r[\"lat16_std\"], r[\"adv_unt\"], r[\"adv_tar\"]\n",
        "            ])\n",
        "\n",
        "    # accuracy vs sparsity plot\n",
        "    sparsities = [r[\"sparsity\"] for r in results]\n",
        "    acc_post = [r[\"acc_post\"] for r in results]\n",
        "    plt.figure()\n",
        "    plt.plot(sparsities, acc_post, marker=\"o\")\n",
        "    plt.xlabel(\"Sparsity\")\n",
        "    plt.ylabel(\"Val accuracy (post finetune)\")\n",
        "    plt.title(\"Accuracy vs Sparsity\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(cfg.out_dir, f\"{cfg.model_type}_acc_vs_sparsity.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # latency vs sparsity (bs=1)\n",
        "    lat1 = [r[\"lat1_mean\"] for r in results]\n",
        "    plt.figure()\n",
        "    plt.plot(sparsities, lat1, marker=\"o\")\n",
        "    plt.xlabel(\"Sparsity\")\n",
        "    plt.ylabel(\"Latency (ms) batch size 1\")\n",
        "    plt.title(\"Latency vs Sparsity\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(cfg.out_dir, f\"{cfg.model_type}_lat_vs_sparsity_bs1.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"\\nSaved summary CSV and plots to {cfg.out_dir}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP9ELT9mZIJs2hfGVt77HB9",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
