{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.ndimage # Added for mask resizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Config\n",
        "# =========================\n",
        "class Config:\n",
        "    data_root = \"data/Sports\"  # expects data_root/train and data_root/val\n",
        "    image_size = 64            # 32 or 64\n",
        "    batch_size = 64\n",
        "    num_workers = 4\n",
        "\n",
        "    num_classes = 10\n",
        "    num_epochs = 10\n",
        "    lr = 1e-3\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    out_dir = \"outputs_problem_a\"\n",
        "    model_a_name = \"simple_cnn\"\n",
        "    model_b_name = \"small_resnet\"\n",
        "\n",
        "    run_name_a = \"run_model_A_simple_cnn\"\n",
        "    run_name_b = \"run_model_B_small_resnet\"\n",
        "\n",
        "\n",
        "cfg = Config()\n",
        "os.makedirs(cfg.out_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Data loaders\n",
        "# =========================\n",
        "def get_dataloaders(cfg: Config):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((cfg.image_size, cfg.image_size)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((cfg.image_size, cfg.image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "\n",
        "    train_dir = os.path.join(cfg.data_root, \"train\")\n",
        "    val_dir = os.path.join(cfg.data_root, \"valid\") # Changed 'val' to 'valid'\n",
        "\n",
        "    train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "    val_dataset = datasets.ImageFolder(val_dir, transform=val_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=cfg.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=cfg.num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    class_names = train_dataset.classes\n",
        "    return train_loader, val_loader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Model A - Simple CNN\n",
        "# =========================\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(2),  # 64 -> 32\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(2),  # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.MaxPool2d(2),  # 16 -> 8\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 8 * 8, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Model B - Small ResNet style\n",
        "# =========================\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=3,\n",
        "            stride=stride, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels, out_channels, kernel_size=3,\n",
        "            stride=1, padding=1, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels, out_channels,\n",
        "                    kernel_size=1, stride=stride, bias=False\n",
        "                ),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(identity)\n",
        "\n",
        "        out = out + identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SmallResNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.layer1 = BasicBlock(32, 64, stride=2)   # 64 -> 32\n",
        "        self.layer2 = BasicBlock(64, 128, stride=2)  # 32 -> 16\n",
        "        self.layer3 = BasicBlock(128, 256, stride=2) # 16 -> 8\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Utility\n",
        "# =========================\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Train and eval loops\n",
        "# =========================\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += preds.eq(targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    return epoch_loss, epoch_acc, epoch_time\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device, num_classes, class_names=None):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    per_class_correct = np.zeros(num_classes, dtype=np.int64)\n",
        "    per_class_total = np.zeros(num_classes, dtype=np.int64)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "\n",
        "            correct += preds.eq(targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "            for t, p in zip(targets, preds):\n",
        "                per_class_total[t.item()] += 1\n",
        "                if t.item() == p.item():\n",
        "                    per_class_correct[t.item()] += 1\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = correct / total\n",
        "    per_class_acc = per_class_correct / np.maximum(per_class_total, 1)\n",
        "\n",
        "    if class_names is not None:\n",
        "        print(\"Per class accuracy:\")\n",
        "        for idx, name in enumerate(class_names):\n",
        "            print(f\"{name}: {per_class_acc[idx] * 100:.2f}% \"\n",
        "                  f\"({per_class_correct[idx]}/{per_class_total[idx]})\")\n",
        "\n",
        "    return epoch_loss, epoch_acc, per_class_acc\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Saliency maps\n",
        "# =========================\n",
        "def compute_saliency_map(model, image_tensor, label, device):\n",
        "    model.eval()\n",
        "    image = image_tensor.unsqueeze(0).to(device)\n",
        "    image.requires_grad_()\n",
        "\n",
        "    output = model(image)\n",
        "    target_score = output[0, label]\n",
        "    model.zero_grad()\n",
        "    target_score.backward()\n",
        "\n",
        "    saliency = image.grad.data.abs().max(dim=1)[0]  # shape (1, H, W)\n",
        "    saliency = saliency.squeeze(0).cpu().numpy() # Now shape (H, W)\n",
        "    saliency = (saliency - saliency.min()) / (saliency.max() - saliency.min() + 1e-8)\n",
        "    return saliency\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Grad CAM\n",
        "# =========================\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "\n",
        "        self.hook_a = target_layer.register_forward_hook(self.save_activation)\n",
        "        self.hook_g = target_layer.register_full_backward_hook(self.save_gradient)\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        self.activations = output.detach()\n",
        "\n",
        "    def save_gradient(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0].detach()\n",
        "\n",
        "    def __call__(self, input_tensor, target_class=None):\n",
        "        self.model.eval()\n",
        "        input_tensor = input_tensor.unsqueeze(0)\n",
        "        input_tensor = input_tensor.to(next(self.model.parameters()).device)\n",
        "        input_tensor.requires_grad_()\n",
        "\n",
        "        output = self.model(input_tensor)\n",
        "        if target_class is None:\n",
        "            target_class = output.argmax(dim=1).item()\n",
        "\n",
        "        loss = output[0, target_class]\n",
        "        self.model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        gradients = self.gradients  # shape (1, C, H, W)\n",
        "        activations = self.activations  # shape (1, C, H, W)\n",
        "\n",
        "        # Corrected: average gradients over spatial dimensions (H, W) for each channel and batch item\n",
        "        weights = gradients.mean(dim=(2, 3), keepdim=True)  # shape (1, C, 1, 1)\n",
        "\n",
        "        # Corrected: multiply weights with activations and sum over channel dimension, then squeeze batch dim\n",
        "        cam = (weights * activations).sum(dim=1)  # shape (1, H, W)\n",
        "        cam = cam.squeeze(0)  # shape (H, W)\n",
        "\n",
        "        cam = cam.cpu().numpy()\n",
        "        cam = np.maximum(cam, 0)\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "        return cam\n",
        "\n",
        "    def close(self):\n",
        "        self.hook_a.remove()\n",
        "        self.hook_g.remove()\n",
        "\n",
        "\n",
        "def show_and_save_heatmap(image_tensor, mask, out_path, alpha=0.4):\n",
        "    # image_tensor is normalized. Need to unnormalize for visualization.\n",
        "    mean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
        "    std = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n",
        "\n",
        "    img = image_tensor.cpu().numpy()\n",
        "    img = std * img + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "    img = np.transpose(img, (1, 2, 0)) # Shape (H, W, C)\n",
        "\n",
        "    # Resize mask to the same dimensions as the image if needed\n",
        "    img_h, img_w, _ = img.shape\n",
        "    mask_h, mask_w = mask.shape\n",
        "\n",
        "    if mask_h != img_h or mask_w != img_w:\n",
        "        zoom_factor_h = img_h / mask_h\n",
        "        zoom_factor_w = img_w / mask_w\n",
        "        # Use order=1 for bilinear interpolation\n",
        "        mask = scipy.ndimage.zoom(mask, (zoom_factor_h, zoom_factor_w), order=1)\n",
        "\n",
        "    heatmap = plt.cm.jet(mask)[..., :3]\n",
        "    overlay = alpha * heatmap + (1 - alpha) * img\n",
        "    overlay = np.clip(overlay, 0, 1)\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(overlay)\n",
        "    plt.tight_layout()\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    plt.savefig(out_path, bbox_inches=\"tight\", pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Training wrapper\n",
        "# =========================\n",
        "def train_and_eval_model(model, model_name, run_name, cfg, train_loader, val_loader, class_names):\n",
        "    device = cfg.device\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    writer = SummaryWriter(log_dir=os.path.join(cfg.out_dir, run_name))\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    num_params = count_parameters(model)\n",
        "    print(f\"Model {model_name} param count: {num_params}\")\n",
        "\n",
        "    for epoch in range(cfg.num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{cfg.num_epochs}\")\n",
        "\n",
        "        train_loss, train_acc, train_time = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, device\n",
        "        )\n",
        "        val_loss, val_acc, _ = evaluate(\n",
        "            model, val_loader, criterion, device,\n",
        "            cfg.num_classes, class_names=None\n",
        "        )\n",
        "\n",
        "        print(f\"Train loss {train_loss:.4f} acc {train_acc:.4f} \"\n",
        "              f\"time {train_time:.2f}s\")\n",
        "        print(f\"Val   loss {val_loss:.4f} acc {val_acc:.4f}\")\n",
        "\n",
        "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
        "        writer.add_scalar(\"Acc/train\", train_acc, epoch)\n",
        "        writer.add_scalar(\"Acc/val\", val_acc, epoch)\n",
        "        writer.add_scalar(\"Time/train_epoch_sec\", train_time, epoch)\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            checkpoint_path = os.path.join(\n",
        "                cfg.out_dir, f\"{model_name}-best-original.pt\"\n",
        "            )\n",
        "            torch.save(best_model_wts, checkpoint_path)\n",
        "            print(f\"Saved best checkpoint to {checkpoint_path}\")\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    final_ckpt_path = os.path.join(\n",
        "        cfg.out_dir, f\"{model_name}-final-original.pt\"\n",
        "    )\n",
        "    torch.save(model.state_dict(), final_ckpt_path)\n",
        "    print(f\"Saved final model to {final_ckpt_path}\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    val_loss, val_acc, per_class_acc = evaluate(\n",
        "        model, val_loader, criterion, device,\n",
        "        cfg.num_classes, class_names=class_names\n",
        "    )\n",
        "\n",
        "    print(f\"Best val accuracy for {model_name}: {best_val_acc:.4f}\")\n",
        "    print(f\"Final val accuracy for {model_name}: {val_acc:.4f}\")\n",
        "\n",
        "    return model, per_class_acc\n",
        "\n",
        "\n",
        "# =========================\n",
        "# Run interpretability\n",
        "# =========================\n",
        "def run_interpretability(model, cfg, class_names, val_loader, model_name):\n",
        "    device = cfg.device\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # choose last conv layer as target for Grad CAM\n",
        "    if isinstance(model, SimpleCNN):\n",
        "        target_layer = model.features[-3]  # last Conv in features\n",
        "    elif isinstance(model, SmallResNet):\n",
        "        target_layer = model.layer3  # last residual block\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model type for Grad CAM\")\n",
        "\n",
        "    grad_cam = GradCAM(model, target_layer)\n",
        "\n",
        "    out_dir = Path(cfg.out_dir) / f\"{model_name}_interpretability\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    images_done = 0\n",
        "    max_images = 10  # you can change\n",
        "\n",
        "    # Removed with torch.no_grad(): block here\n",
        "    for inputs, targets in val_loader:\n",
        "        for i in range(inputs.size(0)):\n",
        "            img = inputs[i]\n",
        "            label = targets[i].item()\n",
        "            label_name = class_names[label]\n",
        "\n",
        "            # Saliency uses grad, so need to re run with grad\n",
        "            sal_map = compute_saliency_map(model, img, label, device)\n",
        "\n",
        "            # Grad CAM\n",
        "            cam = grad_cam(img, target_class=label)\n",
        "\n",
        "            # save saliency\n",
        "            sal_out_path = out_dir / f\"saliency_{images_done}_{label_name}.png\"\n",
        "            plt.figure(figsize=(4, 4))\n",
        "            plt.axis(\"off\")\n",
        "            plt.imshow(sal_map, cmap=\"hot\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(sal_out_path, bbox_inches=\"tight\", pad_inches=0)\n",
        "            plt.close()\n",
        "\n",
        "            # save Grad CAM overlay\n",
        "            cam_out_path = out_dir / f\"gradcam_{images_done}_{label_name}.png\"\n",
        "            show_and_save_heatmap(img, cam, str(cam_out_path))\n",
        "\n",
        "            images_done += 1\n",
        "            if images_done >= max_images:\n",
        "                grad_cam.close()\n",
        "                print(f\"Saved {images_done} interpretability images for {model_name}\")\n",
        "                return\n",
        "\n",
        "    grad_cam.close()\n",
        "    print(f\"Saved {images_done} interpretability images for {model_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APjDR0qx19Hh",
        "outputId": "96b5e1bb-6808-4f18-9923-6dd93f4f05e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Model A - SimpleCNN\n",
            "Model simple_cnn param count: 4586506\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss 4.0491 acc 0.2241 time 59.64s\n",
            "Val   loss 1.9434 acc 0.3800\n",
            "Saved best checkpoint to outputs_problem_a/simple_cnn-best-original.pt\n",
            "Epoch 2/10\n",
            "Train loss 1.8904 acc 0.3315 time 59.16s\n",
            "Val   loss 1.5749 acc 0.4600\n",
            "Saved best checkpoint to outputs_problem_a/simple_cnn-best-original.pt\n",
            "Epoch 3/10\n",
            "Train loss 1.7181 acc 0.3810 time 60.55s\n",
            "Val   loss 1.4824 acc 0.4800\n",
            "Saved best checkpoint to outputs_problem_a/simple_cnn-best-original.pt\n",
            "Epoch 4/10\n",
            "Train loss 1.6580 acc 0.4244 time 60.56s\n",
            "Val   loss 1.5847 acc 0.4200\n",
            "Epoch 5/10\n",
            "Train loss 1.5660 acc 0.4338 time 67.51s\n",
            "Val   loss 1.3349 acc 0.4800\n",
            "Epoch 6/10\n",
            "Train loss 1.5080 acc 0.4721 time 60.28s\n",
            "Val   loss 1.3385 acc 0.5400\n",
            "Saved best checkpoint to outputs_problem_a/simple_cnn-best-original.pt\n",
            "Epoch 7/10\n",
            "Train loss 1.4211 acc 0.4765 time 61.55s\n",
            "Val   loss 1.3122 acc 0.4600\n",
            "Epoch 8/10\n",
            "Train loss 1.4037 acc 0.5116 time 61.01s\n",
            "Val   loss 1.2654 acc 0.5800\n",
            "Saved best checkpoint to outputs_problem_a/simple_cnn-best-original.pt\n",
            "Epoch 9/10\n",
            "Train loss 1.3927 acc 0.4928 time 61.98s\n",
            "Val   loss 1.2527 acc 0.5600\n",
            "Epoch 10/10\n",
            "Train loss 1.3267 acc 0.5248 time 60.34s\n",
            "Val   loss 1.1765 acc 0.5600\n",
            "Saved final model to outputs_problem_a/simple_cnn-final-original.pt\n",
            "Per class accuracy:\n",
            "baseball: 20.00% (1/5)\n",
            "basketball: 40.00% (2/5)\n",
            "football: 40.00% (2/5)\n",
            "golf: 80.00% (4/5)\n",
            "hockey: 100.00% (5/5)\n",
            "rugby: 40.00% (2/5)\n",
            "swimming: 100.00% (5/5)\n",
            "tennis: 60.00% (3/5)\n",
            "volleyball: 40.00% (2/5)\n",
            "weightlifting: 60.00% (3/5)\n",
            "Best val accuracy for simple_cnn: 0.5800\n",
            "Final val accuracy for simple_cnn: 0.5800\n",
            "Saved 10 interpretability images for simple_cnn\n",
            "\n",
            "Training Model B - SmallResNet\n",
            "Model small_resnet param count: 1210410\n",
            "Epoch 1/10\n",
            "Train loss 1.7391 acc 0.3942 time 46.12s\n",
            "Val   loss 1.4815 acc 0.5200\n",
            "Saved best checkpoint to outputs_problem_a/small_resnet-best-original.pt\n",
            "Epoch 2/10\n",
            "Train loss 1.3659 acc 0.5292 time 45.78s\n",
            "Val   loss 1.4260 acc 0.5800\n",
            "Saved best checkpoint to outputs_problem_a/small_resnet-best-original.pt\n",
            "Epoch 3/10\n",
            "Train loss 1.2151 acc 0.5913 time 44.14s\n",
            "Val   loss 1.4261 acc 0.5000\n",
            "Epoch 4/10\n",
            "Train loss 1.1104 acc 0.6177 time 45.16s\n",
            "Val   loss 1.1273 acc 0.6000\n",
            "Saved best checkpoint to outputs_problem_a/small_resnet-best-original.pt\n",
            "Epoch 5/10\n",
            "Train loss 1.0102 acc 0.6579 time 45.70s\n",
            "Val   loss 0.9509 acc 0.6600\n",
            "Saved best checkpoint to outputs_problem_a/small_resnet-best-original.pt\n",
            "Epoch 6/10\n",
            "Train loss 0.9885 acc 0.6616 time 48.27s\n",
            "Val   loss 0.9322 acc 0.6600\n",
            "Epoch 7/10\n",
            "Train loss 0.9473 acc 0.6786 time 44.54s\n",
            "Val   loss 0.8968 acc 0.6800\n",
            "Saved best checkpoint to outputs_problem_a/small_resnet-best-original.pt\n",
            "Epoch 8/10\n",
            "Train loss 0.8487 acc 0.7100 time 44.93s\n",
            "Val   loss 0.8916 acc 0.7000\n",
            "Saved best checkpoint to outputs_problem_a/small_resnet-best-original.pt\n",
            "Epoch 9/10\n",
            "Train loss 0.8107 acc 0.7257 time 45.26s\n",
            "Val   loss 0.7494 acc 0.7800\n",
            "Saved best checkpoint to outputs_problem_a/small_resnet-best-original.pt\n",
            "Epoch 10/10\n",
            "Train loss 0.8162 acc 0.7244 time 45.38s\n",
            "Val   loss 0.9126 acc 0.7200\n",
            "Saved final model to outputs_problem_a/small_resnet-final-original.pt\n",
            "Per class accuracy:\n",
            "baseball: 80.00% (4/5)\n",
            "basketball: 60.00% (3/5)\n",
            "football: 80.00% (4/5)\n",
            "golf: 100.00% (5/5)\n",
            "hockey: 100.00% (5/5)\n",
            "rugby: 80.00% (4/5)\n",
            "swimming: 100.00% (5/5)\n",
            "tennis: 80.00% (4/5)\n",
            "volleyball: 20.00% (1/5)\n",
            "weightlifting: 80.00% (4/5)\n",
            "Best val accuracy for small_resnet: 0.7800\n",
            "Final val accuracy for small_resnet: 0.7800\n",
            "Saved 10 interpretability images for small_resnet\n",
            "Done\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =========================\n",
        "# Main\n",
        "# =========================\n",
        "def main():\n",
        "    train_loader, val_loader, class_names = get_dataloaders(cfg)\n",
        "\n",
        "    print(\"Training Model A - SimpleCNN\")\n",
        "    model_a = SimpleCNN(num_classes=cfg.num_classes)\n",
        "    model_a, per_class_acc_a = train_and_eval_model(\n",
        "        model_a,\n",
        "        cfg.model_a_name,\n",
        "        cfg.run_name_a,\n",
        "        cfg,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        class_names\n",
        "    )\n",
        "    run_interpretability(model_a, cfg, class_names, val_loader, cfg.model_a_name)\n",
        "\n",
        "    print(\"\\nTraining Model B - SmallResNet\")\n",
        "    model_b = SmallResNet(num_classes=cfg.num_classes)\n",
        "    model_b, per_class_acc_b = train_and_eval_model(\n",
        "        model_b,\n",
        "        cfg.model_b_name,\n",
        "        cfg.run_name_b,\n",
        "        cfg,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        class_names\n",
        "    )\n",
        "    run_interpretability(model_b, cfg, class_names, val_loader, cfg.model_b_name)\n",
        "\n",
        "    print(\"Done\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP9ELT9mZIJs2hfGVt77HB9",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
